{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Linear\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "from torch_geometric.nn import global_mean_pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "triangle_array = np.array([[0, 1, 0, 2, 1, 2],\n",
    "                           [1, 0, 2, 0, 2, 1]])\n",
    "edge_index_triangle = torch.tensor(triangle_array, dtype=torch.long)\n",
    "x_triangle = torch.tensor([[1], [1], [1]], dtype=torch.float)\n",
    "data_triangle = Data(x=x_triangle, edge_index=edge_index_triangle)\n",
    "\n",
    "square_array = np.array([[0, 1, 0, 3, 1, 2, 2, 3],\n",
    "                         [1, 0, 3, 0, 2, 1, 3, 2]])\n",
    "edge_index_square = torch.tensor(square_array, dtype=torch.long)\n",
    "x_square = torch.tensor([[1], [1], [1], [1]], dtype=torch.float)\n",
    "data_square = Data(x=x_square, edge_index=edge_index_square)\n",
    "\n",
    "penta_array = np.array([[0, 1, 0, 4, 1, 2, 2, 3, 3, 4],\n",
    "                        [1, 0, 4, 0, 2, 1, 3, 2, 4, 3]])\n",
    "edge_index_penta = torch.tensor(penta_array, dtype=torch.long)\n",
    "x_penta = torch.tensor([[1], [1], [1], [1], [1]], dtype=torch.float)\n",
    "data_penta = Data(x=x_penta, edge_index=edge_index_penta)\n",
    "\n",
    "\n",
    "star4_array = np.array([[0, 1, 1, 2, 1, 3],\n",
    "                        [1, 0, 2, 1, 3, 1]])\n",
    "edge_index_star4 = torch.tensor(star4_array, dtype=torch.long)\n",
    "x_star4 = torch.tensor([[1], [1], [1], [1]], dtype=torch.float)\n",
    "data_star = Data(x=x_star4, edge_index=edge_index_star4)\n",
    "\n",
    "star5_array = np.array([[0, 1, 1, 2, 1, 3, 1, 4],\n",
    "                        [1, 0, 2, 1, 3, 1, 4, 1]])\n",
    "edge_index_star5 = torch.tensor(star4_array, dtype=torch.long)\n",
    "x_star5 = torch.tensor([[1], [1], [1], [1], [1]], dtype=torch.float)\n",
    "data_star5 = Data(x=x_star5, edge_index=edge_index_star5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_renamings(vertex_ls, graph_arr):\n",
    "    all_renamed = []\n",
    "    renames = itertools.permutations(vertex_ls)\n",
    "    for rename in renames:\n",
    "        renaming_dict = {i: j for i, j in zip(vertex_ls, rename)}\n",
    "        renamed = np.zeros(graph_arr.shape)\n",
    "        for i, row in enumerate(graph_arr):\n",
    "            renamed[i] = np.array([renaming_dict.get(number) for number in row])\n",
    "        all_renamed.append(renamed)\n",
    "    return all_renamed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\n",
      "120\n",
      "24\n",
      "120\n",
      "length of dataset: 288\n"
     ]
    }
   ],
   "source": [
    "dataset = []\n",
    "labels = []\n",
    "\n",
    "# print(len(generate_renamings([0,1,2], triangle_array)))\n",
    "# for renaming in generate_renamings([0,1,2], triangle_array):\n",
    "#     edge_index_triangle = torch.tensor(renaming, dtype=torch.long)\n",
    "#     data_triangle = Data(x=x_triangle, edge_index=edge_index_triangle, y=0)\n",
    "#     dataset.append(data_triangle)\n",
    "    \n",
    "print(len(generate_renamings([0,1,2,3], square_array)))\n",
    "for renaming in generate_renamings([0,1,2,3], square_array):\n",
    "    edge_index_square = torch.tensor(renaming, dtype=torch.long)\n",
    "    data_square = Data(x=x_square, edge_index=edge_index_square, y=0)\n",
    "    dataset.append(data_square)\n",
    "\n",
    "print(len(generate_renamings([0,1,2,3,4], penta_array)))\n",
    "for renaming in generate_renamings([0,1,2,3,4], penta_array):\n",
    "    edge_index_penta = torch.tensor(renaming, dtype=torch.long)\n",
    "    data_penta = Data(x=x_penta, edge_index=edge_index_penta, y=1)\n",
    "    dataset.append(data_penta)    \n",
    "    \n",
    "print(len(generate_renamings([0,1,2,3], star4_array)))\n",
    "for renaming in generate_renamings([0,1,2,3], star4_array):\n",
    "    edge_index_star4 = torch.tensor(renaming, dtype=torch.long)\n",
    "    data_star4 = Data(x=x_star4, edge_index=edge_index_star4, y=2)\n",
    "    dataset.append(data_star4)\n",
    "\n",
    "print(len(generate_renamings([0,1,2,3,4], star5_array)))\n",
    "for renaming in generate_renamings([0,1,2,3,4], star5_array):\n",
    "    edge_index_star5 = torch.tensor(renaming, dtype=torch.long)\n",
    "    data_star5 = Data(x=x_star5, edge_index=edge_index_star5, y=3)\n",
    "    dataset.append(data_star5)\n",
    "\n",
    "print('length of dataset:', len(dataset))\n",
    "train_size = round(0.8*len(dataset))\n",
    "dataset = random.sample(dataset, len(dataset))\n",
    "# random.shuffle(dataset)\n",
    "\n",
    "train_data = dataset[:train_size]\n",
    "test_data = dataset[train_size:]\n",
    "# loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "train_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCN(\n",
      "  (conv1): GCNConv(1, 64)\n",
      "  (conv2): GCNConv(64, 64)\n",
      "  (conv3): GCNConv(64, 64)\n",
      "  (lin): Linear(in_features=64, out_features=4, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super(GCN, self).__init__()\n",
    "        torch.manual_seed(12345)\n",
    "        self.conv1 = GCNConv(1, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.conv3 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.lin = Linear(hidden_channels, 4)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        # 1. Obtain node embeddings \n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.conv3(x, edge_index)\n",
    "\n",
    "        # 2. Readout layer\n",
    "        x = global_mean_pool(x, batch)  # [batch_size, hidden_channels]\n",
    "\n",
    "        # 3. Apply a final classifier\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.lin(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "model = GCN(hidden_channels=64)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Train Acc: 0.4174, Test Acc: 0.4138\n",
      "Epoch: 002, Train Acc: 0.4174, Test Acc: 0.4138\n",
      "Epoch: 003, Train Acc: 0.4174, Test Acc: 0.4138\n",
      "Epoch: 004, Train Acc: 0.4174, Test Acc: 0.4138\n",
      "Epoch: 005, Train Acc: 0.4174, Test Acc: 0.4138\n",
      "Epoch: 006, Train Acc: 0.4174, Test Acc: 0.4138\n",
      "Epoch: 007, Train Acc: 0.4174, Test Acc: 0.4138\n",
      "Epoch: 008, Train Acc: 0.4174, Test Acc: 0.4138\n",
      "Epoch: 009, Train Acc: 0.4174, Test Acc: 0.4138\n",
      "Epoch: 010, Train Acc: 0.4174, Test Acc: 0.4138\n",
      "Epoch: 011, Train Acc: 0.4174, Test Acc: 0.4138\n",
      "Epoch: 012, Train Acc: 0.4174, Test Acc: 0.4138\n",
      "Epoch: 013, Train Acc: 0.4174, Test Acc: 0.4138\n",
      "Epoch: 014, Train Acc: 0.4174, Test Acc: 0.4138\n",
      "Epoch: 015, Train Acc: 0.4174, Test Acc: 0.4138\n",
      "Epoch: 016, Train Acc: 0.4174, Test Acc: 0.4138\n",
      "Epoch: 017, Train Acc: 0.4174, Test Acc: 0.4138\n",
      "Epoch: 018, Train Acc: 0.4174, Test Acc: 0.4138\n",
      "Epoch: 019, Train Acc: 0.4174, Test Acc: 0.4138\n",
      "Epoch: 020, Train Acc: 0.4174, Test Acc: 0.4138\n",
      "Epoch: 021, Train Acc: 0.4174, Test Acc: 0.4138\n",
      "Epoch: 022, Train Acc: 0.4174, Test Acc: 0.4138\n",
      "Epoch: 023, Train Acc: 0.4174, Test Acc: 0.4138\n",
      "Epoch: 024, Train Acc: 0.4174, Test Acc: 0.4138\n",
      "Epoch: 025, Train Acc: 0.4174, Test Acc: 0.4138\n",
      "Epoch: 026, Train Acc: 0.4174, Test Acc: 0.4138\n",
      "Epoch: 027, Train Acc: 0.8348, Test Acc: 0.8276\n",
      "Epoch: 028, Train Acc: 0.4174, Test Acc: 0.4138\n",
      "Epoch: 029, Train Acc: 0.8348, Test Acc: 0.8276\n",
      "Epoch: 030, Train Acc: 0.8348, Test Acc: 0.8276\n",
      "Epoch: 031, Train Acc: 0.4174, Test Acc: 0.4138\n",
      "Epoch: 032, Train Acc: 0.8348, Test Acc: 0.8276\n",
      "Epoch: 033, Train Acc: 0.8348, Test Acc: 0.8276\n",
      "Epoch: 034, Train Acc: 0.8348, Test Acc: 0.8276\n",
      "Epoch: 035, Train Acc: 0.8348, Test Acc: 0.8276\n",
      "Epoch: 036, Train Acc: 0.8348, Test Acc: 0.8276\n",
      "Epoch: 037, Train Acc: 0.8348, Test Acc: 0.8276\n",
      "Epoch: 038, Train Acc: 0.8348, Test Acc: 0.8276\n",
      "Epoch: 039, Train Acc: 0.8348, Test Acc: 0.8276\n",
      "Epoch: 040, Train Acc: 0.8348, Test Acc: 0.8276\n",
      "Epoch: 041, Train Acc: 0.8348, Test Acc: 0.8276\n",
      "Epoch: 042, Train Acc: 0.8348, Test Acc: 0.8276\n",
      "Epoch: 043, Train Acc: 0.8348, Test Acc: 0.8276\n",
      "Epoch: 044, Train Acc: 0.8348, Test Acc: 0.8276\n",
      "Epoch: 045, Train Acc: 0.8348, Test Acc: 0.8276\n",
      "Epoch: 046, Train Acc: 0.8348, Test Acc: 0.8276\n",
      "Epoch: 047, Train Acc: 0.8348, Test Acc: 0.8276\n",
      "Epoch: 048, Train Acc: 0.8348, Test Acc: 0.8276\n",
      "Epoch: 049, Train Acc: 0.8348, Test Acc: 0.8276\n",
      "Epoch: 050, Train Acc: 0.9261, Test Acc: 0.8793\n",
      "Epoch: 051, Train Acc: 0.8348, Test Acc: 0.8276\n",
      "Epoch: 052, Train Acc: 0.9261, Test Acc: 0.8793\n",
      "Epoch: 053, Train Acc: 0.9261, Test Acc: 0.8793\n",
      "Epoch: 054, Train Acc: 0.9261, Test Acc: 0.8793\n",
      "Epoch: 055, Train Acc: 0.9261, Test Acc: 0.8793\n",
      "Epoch: 056, Train Acc: 0.9261, Test Acc: 0.8793\n",
      "Epoch: 057, Train Acc: 0.9261, Test Acc: 0.8793\n",
      "Epoch: 058, Train Acc: 0.8348, Test Acc: 0.8276\n",
      "Epoch: 059, Train Acc: 0.9261, Test Acc: 0.8793\n",
      "Epoch: 060, Train Acc: 0.9261, Test Acc: 0.8793\n",
      "Epoch: 061, Train Acc: 0.9261, Test Acc: 0.8793\n",
      "Epoch: 062, Train Acc: 0.9261, Test Acc: 0.8793\n",
      "Epoch: 063, Train Acc: 0.9261, Test Acc: 0.8793\n",
      "Epoch: 064, Train Acc: 0.9261, Test Acc: 0.8793\n",
      "Epoch: 065, Train Acc: 0.9261, Test Acc: 0.8793\n",
      "Epoch: 066, Train Acc: 0.9261, Test Acc: 0.8793\n",
      "Epoch: 067, Train Acc: 0.9261, Test Acc: 0.8793\n",
      "Epoch: 068, Train Acc: 0.9261, Test Acc: 0.8793\n",
      "Epoch: 069, Train Acc: 0.9261, Test Acc: 0.8793\n",
      "Epoch: 070, Train Acc: 0.9261, Test Acc: 0.8793\n",
      "Epoch: 071, Train Acc: 0.9261, Test Acc: 0.8793\n",
      "Epoch: 072, Train Acc: 0.9261, Test Acc: 0.8793\n",
      "Epoch: 073, Train Acc: 0.9261, Test Acc: 0.8793\n",
      "Epoch: 074, Train Acc: 0.9261, Test Acc: 0.8793\n",
      "Epoch: 075, Train Acc: 0.9261, Test Acc: 0.8793\n",
      "Epoch: 076, Train Acc: 0.9261, Test Acc: 0.8793\n",
      "Epoch: 077, Train Acc: 0.9261, Test Acc: 0.8793\n",
      "Epoch: 078, Train Acc: 0.9261, Test Acc: 0.8793\n",
      "Epoch: 079, Train Acc: 0.9261, Test Acc: 0.8793\n",
      "Epoch: 080, Train Acc: 0.9261, Test Acc: 0.8793\n",
      "Epoch: 081, Train Acc: 0.9261, Test Acc: 0.8793\n",
      "Epoch: 082, Train Acc: 0.9261, Test Acc: 0.8793\n",
      "Epoch: 083, Train Acc: 0.9261, Test Acc: 0.8793\n",
      "Epoch: 084, Train Acc: 0.9261, Test Acc: 0.8793\n",
      "Epoch: 085, Train Acc: 0.9261, Test Acc: 0.8793\n",
      "Epoch: 086, Train Acc: 0.9261, Test Acc: 0.8793\n",
      "Epoch: 087, Train Acc: 0.9261, Test Acc: 0.8793\n",
      "Epoch: 088, Train Acc: 0.9261, Test Acc: 0.8793\n",
      "Epoch: 089, Train Acc: 0.9261, Test Acc: 0.8793\n",
      "Epoch: 090, Train Acc: 0.9261, Test Acc: 0.8793\n",
      "Epoch: 091, Train Acc: 0.9261, Test Acc: 0.8793\n",
      "Epoch: 092, Train Acc: 0.9261, Test Acc: 0.8793\n",
      "Epoch: 093, Train Acc: 0.9261, Test Acc: 0.8793\n",
      "Epoch: 094, Train Acc: 0.9261, Test Acc: 0.8793\n",
      "Epoch: 095, Train Acc: 0.9261, Test Acc: 0.8793\n",
      "Epoch: 096, Train Acc: 0.9261, Test Acc: 0.8793\n",
      "Epoch: 097, Train Acc: 0.9261, Test Acc: 0.8793\n",
      "Epoch: 098, Train Acc: 0.9261, Test Acc: 0.8793\n",
      "Epoch: 099, Train Acc: 0.9261, Test Acc: 0.8793\n",
      "Epoch: 100, Train Acc: 0.9261, Test Acc: 0.8793\n",
      "Epoch: 101, Train Acc: 0.9261, Test Acc: 0.8793\n",
      "Epoch: 102, Train Acc: 0.9261, Test Acc: 0.8793\n",
      "Epoch: 103, Train Acc: 0.9261, Test Acc: 0.8793\n",
      "Epoch: 104, Train Acc: 0.9261, Test Acc: 0.8793\n",
      "Epoch: 105, Train Acc: 0.9261, Test Acc: 0.8793\n",
      "Epoch: 106, Train Acc: 0.9261, Test Acc: 0.8793\n",
      "Epoch: 107, Train Acc: 0.9261, Test Acc: 0.8793\n",
      "Epoch: 108, Train Acc: 0.9261, Test Acc: 0.8793\n",
      "Epoch: 109, Train Acc: 0.9261, Test Acc: 0.8793\n",
      "Epoch: 110, Train Acc: 0.9261, Test Acc: 0.8793\n",
      "Epoch: 111, Train Acc: 0.9261, Test Acc: 0.8793\n",
      "Epoch: 112, Train Acc: 0.9261, Test Acc: 0.8793\n",
      "Epoch: 113, Train Acc: 0.9261, Test Acc: 0.8793\n",
      "Epoch: 114, Train Acc: 0.9261, Test Acc: 0.8793\n",
      "Epoch: 115, Train Acc: 0.9261, Test Acc: 0.8793\n",
      "Epoch: 116, Train Acc: 0.9261, Test Acc: 0.8793\n",
      "Epoch: 117, Train Acc: 0.9261, Test Acc: 0.8793\n",
      "Epoch: 118, Train Acc: 0.9261, Test Acc: 0.8793\n",
      "Epoch: 119, Train Acc: 0.9261, Test Acc: 0.8793\n",
      "Epoch: 120, Train Acc: 0.9261, Test Acc: 0.8793\n",
      "Epoch: 121, Train Acc: 0.9261, Test Acc: 0.8793\n",
      "Epoch: 122, Train Acc: 0.9261, Test Acc: 0.8793\n",
      "Epoch: 123, Train Acc: 0.9261, Test Acc: 0.8793\n",
      "Epoch: 124, Train Acc: 0.9261, Test Acc: 0.8793\n",
      "Epoch: 125, Train Acc: 0.9261, Test Acc: 0.8793\n",
      "Epoch: 126, Train Acc: 0.9261, Test Acc: 0.8793\n",
      "Epoch: 127, Train Acc: 0.9261, Test Acc: 0.8793\n",
      "Epoch: 128, Train Acc: 0.9261, Test Acc: 0.8793\n",
      "Epoch: 129, Train Acc: 0.9261, Test Acc: 0.8793\n",
      "Epoch: 130, Train Acc: 0.9261, Test Acc: 0.8793\n",
      "Epoch: 131, Train Acc: 0.9261, Test Acc: 0.8793\n",
      "Epoch: 132, Train Acc: 0.9261, Test Acc: 0.8793\n",
      "Epoch: 133, Train Acc: 0.9261, Test Acc: 0.8793\n",
      "Epoch: 134, Train Acc: 0.9261, Test Acc: 0.8793\n",
      "Epoch: 135, Train Acc: 0.9261, Test Acc: 0.8793\n",
      "Epoch: 136, Train Acc: 0.9261, Test Acc: 0.8793\n",
      "Epoch: 137, Train Acc: 0.9261, Test Acc: 0.8793\n",
      "Epoch: 138, Train Acc: 0.9261, Test Acc: 0.8793\n",
      "Epoch: 139, Train Acc: 0.9261, Test Acc: 0.8793\n",
      "Epoch: 140, Train Acc: 0.9261, Test Acc: 0.8793\n",
      "Epoch: 141, Train Acc: 0.9261, Test Acc: 0.8793\n",
      "Epoch: 142, Train Acc: 0.9261, Test Acc: 0.8793\n",
      "Epoch: 143, Train Acc: 0.9261, Test Acc: 0.8793\n",
      "Epoch: 144, Train Acc: 0.9261, Test Acc: 0.8793\n",
      "Epoch: 145, Train Acc: 0.9261, Test Acc: 0.8793\n",
      "Epoch: 146, Train Acc: 0.9261, Test Acc: 0.8793\n",
      "Epoch: 147, Train Acc: 0.9261, Test Acc: 0.8793\n",
      "Epoch: 148, Train Acc: 0.9261, Test Acc: 0.8793\n",
      "Epoch: 149, Train Acc: 0.9261, Test Acc: 0.8793\n",
      "Epoch: 150, Train Acc: 0.9261, Test Acc: 0.8793\n",
      "Epoch: 151, Train Acc: 0.9261, Test Acc: 0.8793\n",
      "Epoch: 152, Train Acc: 0.9261, Test Acc: 0.8793\n",
      "Epoch: 153, Train Acc: 0.9261, Test Acc: 0.8793\n",
      "Epoch: 154, Train Acc: 0.9261, Test Acc: 0.8793\n",
      "Epoch: 155, Train Acc: 0.9261, Test Acc: 0.8793\n",
      "Epoch: 156, Train Acc: 0.9261, Test Acc: 0.8793\n",
      "Epoch: 157, Train Acc: 0.9261, Test Acc: 0.8793\n",
      "Epoch: 158, Train Acc: 0.9261, Test Acc: 0.8793\n",
      "Epoch: 159, Train Acc: 0.9261, Test Acc: 0.8793\n",
      "Epoch: 160, Train Acc: 0.9261, Test Acc: 0.8793\n",
      "Epoch: 161, Train Acc: 0.9261, Test Acc: 0.8793\n",
      "Epoch: 162, Train Acc: 0.9261, Test Acc: 0.8793\n",
      "Epoch: 163, Train Acc: 0.9261, Test Acc: 0.8793\n",
      "Epoch: 164, Train Acc: 0.9261, Test Acc: 0.8793\n",
      "Epoch: 165, Train Acc: 0.9261, Test Acc: 0.8793\n",
      "Epoch: 166, Train Acc: 0.9261, Test Acc: 0.8793\n",
      "Epoch: 167, Train Acc: 0.9261, Test Acc: 0.8793\n",
      "Epoch: 168, Train Acc: 0.9261, Test Acc: 0.8793\n",
      "Epoch: 169, Train Acc: 0.9261, Test Acc: 0.8793\n",
      "Epoch: 170, Train Acc: 0.9261, Test Acc: 0.8793\n",
      "Epoch: 171, Train Acc: 0.9261, Test Acc: 0.8793\n",
      "Epoch: 172, Train Acc: 0.9261, Test Acc: 0.8793\n",
      "Epoch: 173, Train Acc: 0.9261, Test Acc: 0.8793\n",
      "Epoch: 174, Train Acc: 0.9261, Test Acc: 0.8793\n",
      "Epoch: 175, Train Acc: 0.9261, Test Acc: 0.8793\n",
      "Epoch: 176, Train Acc: 0.9261, Test Acc: 0.8793\n",
      "Epoch: 177, Train Acc: 0.9261, Test Acc: 0.8793\n",
      "Epoch: 178, Train Acc: 0.9261, Test Acc: 0.8793\n",
      "Epoch: 179, Train Acc: 0.9261, Test Acc: 0.8793\n",
      "Epoch: 180, Train Acc: 0.9261, Test Acc: 0.8793\n",
      "Epoch: 181, Train Acc: 0.9261, Test Acc: 0.8793\n",
      "Epoch: 182, Train Acc: 0.9261, Test Acc: 0.8793\n",
      "Epoch: 183, Train Acc: 0.9261, Test Acc: 0.8793\n",
      "Epoch: 184, Train Acc: 0.9261, Test Acc: 0.8793\n",
      "Epoch: 185, Train Acc: 0.9261, Test Acc: 0.8793\n",
      "Epoch: 186, Train Acc: 0.9261, Test Acc: 0.8793\n",
      "Epoch: 187, Train Acc: 0.9261, Test Acc: 0.8793\n",
      "Epoch: 188, Train Acc: 0.9261, Test Acc: 0.8793\n",
      "Epoch: 189, Train Acc: 0.9261, Test Acc: 0.8793\n",
      "Epoch: 190, Train Acc: 0.9261, Test Acc: 0.8793\n",
      "Epoch: 191, Train Acc: 0.9261, Test Acc: 0.8793\n",
      "Epoch: 192, Train Acc: 0.9261, Test Acc: 0.8793\n",
      "Epoch: 193, Train Acc: 0.9261, Test Acc: 0.8793\n",
      "Epoch: 194, Train Acc: 0.9261, Test Acc: 0.8793\n",
      "Epoch: 195, Train Acc: 0.9261, Test Acc: 0.8793\n",
      "Epoch: 196, Train Acc: 0.9261, Test Acc: 0.8793\n",
      "Epoch: 197, Train Acc: 0.9261, Test Acc: 0.8793\n",
      "Epoch: 198, Train Acc: 0.9261, Test Acc: 0.8793\n",
      "Epoch: 199, Train Acc: 0.9261, Test Acc: 0.8793\n",
      "Epoch: 200, Train Acc: 0.9261, Test Acc: 0.8793\n",
      "17\n",
      "tensor([[0, 2, 0, 3, 2, 1, 1, 3],\n",
      "        [2, 0, 3, 0, 1, 2, 3, 1]])\n",
      "tensor([0])\n",
      "tensor([[2, 0, 2, 1, 0, 3, 3, 1],\n",
      "        [0, 2, 1, 2, 3, 0, 1, 3]])\n",
      "tensor([0])\n",
      "tensor([[2, 3, 2, 0, 3, 1, 1, 0],\n",
      "        [3, 2, 0, 2, 1, 3, 0, 1]])\n",
      "tensor([0])\n",
      "tensor([[1, 0, 1, 3, 0, 2, 2, 3],\n",
      "        [0, 1, 3, 1, 2, 0, 3, 2]])\n",
      "tensor([0])\n",
      "tensor([[3, 2, 3, 0, 2, 1, 1, 0],\n",
      "        [2, 3, 0, 3, 1, 2, 0, 1]])\n",
      "tensor([0])\n",
      "tensor([[2, 3, 2, 1, 3, 0, 0, 1],\n",
      "        [3, 2, 1, 2, 0, 3, 1, 0]])\n",
      "tensor([0])\n",
      "tensor([[2, 0, 2, 3, 0, 1, 1, 3],\n",
      "        [0, 2, 3, 2, 1, 0, 3, 1]])\n",
      "tensor([0])\n",
      "tensor([[3, 1, 3, 2, 1, 0, 0, 2],\n",
      "        [1, 3, 2, 3, 0, 1, 2, 0]])\n",
      "tensor([0])\n",
      "tensor([[0, 3, 0, 1, 3, 2, 2, 1],\n",
      "        [3, 0, 1, 0, 2, 3, 1, 2]])\n",
      "tensor([0])\n",
      "tensor([[0, 2, 0, 1, 2, 3, 3, 1],\n",
      "        [2, 0, 1, 0, 3, 2, 1, 3]])\n",
      "tensor([0])\n",
      "tensor([[1, 2, 1, 3, 2, 0, 0, 3],\n",
      "        [2, 1, 3, 1, 0, 2, 3, 0]])\n",
      "tensor([0])\n",
      "tensor([[2, 1, 2, 3, 1, 0, 0, 3],\n",
      "        [1, 2, 3, 2, 0, 1, 3, 0]])\n",
      "tensor([0])\n",
      "tensor([[0, 1, 0, 3, 1, 2, 2, 3],\n",
      "        [1, 0, 3, 0, 2, 1, 3, 2]])\n",
      "tensor([0])\n",
      "tensor([[3, 2, 3, 1, 2, 0, 0, 1],\n",
      "        [2, 3, 1, 3, 0, 2, 1, 0]])\n",
      "tensor([0])\n",
      "tensor([[1, 3, 1, 2, 3, 0, 0, 2],\n",
      "        [3, 1, 2, 1, 0, 3, 2, 0]])\n",
      "tensor([0])\n",
      "tensor([[0, 1, 0, 2, 1, 3, 3, 2],\n",
      "        [1, 0, 2, 0, 3, 1, 2, 3]])\n",
      "tensor([0])\n",
      "tensor([[1, 0, 1, 2, 0, 3, 3, 2],\n",
      "        [0, 1, 2, 1, 3, 0, 2, 3]])\n",
      "tensor([0])\n",
      "7\n",
      "tensor([[1, 2, 1, 0, 2, 3, 3, 0],\n",
      "        [2, 1, 0, 1, 3, 2, 0, 3]])\n",
      "tensor([0])\n",
      "tensor([[2, 1, 2, 0, 1, 3, 3, 0],\n",
      "        [1, 2, 0, 2, 3, 1, 0, 3]])\n",
      "tensor([0])\n",
      "tensor([[0, 3, 0, 2, 3, 1, 1, 2],\n",
      "        [3, 0, 2, 0, 1, 3, 2, 1]])\n",
      "tensor([0])\n",
      "tensor([[3, 0, 3, 2, 0, 1, 1, 2],\n",
      "        [0, 3, 2, 3, 1, 0, 2, 1]])\n",
      "tensor([0])\n",
      "tensor([[3, 1, 3, 0, 1, 2, 2, 0],\n",
      "        [1, 3, 0, 3, 2, 1, 0, 2]])\n",
      "tensor([0])\n",
      "tensor([[3, 0, 3, 1, 0, 2, 2, 1],\n",
      "        [0, 3, 1, 3, 2, 0, 1, 2]])\n",
      "tensor([0])\n",
      "tensor([[1, 3, 1, 0, 3, 2, 2, 0],\n",
      "        [3, 1, 0, 1, 2, 3, 0, 2]])\n",
      "tensor([0])\n"
     ]
    }
   ],
   "source": [
    "model = GCN(hidden_channels=64)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "\n",
    "    for data in train_loader:  # Iterate in batches over the training dataset.\n",
    "        out = model(data.x, data.edge_index, data.batch)  # Perform a single forward pass.\n",
    "        loss = criterion(out, data.y)  # Compute the loss.\n",
    "        loss.backward()  # Derive gradients.\n",
    "        optimizer.step()  # Update parameters based on gradients.\n",
    "        optimizer.zero_grad()  # Clear gradients.\n",
    "#     print(loss.item())\n",
    "\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "    misclass = []\n",
    "\n",
    "    correct = 0\n",
    "    for data in loader:  # Iterate in batches over the training/test dataset.\n",
    "        out = model(data.x, data.edge_index, data.batch)  \n",
    "        pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
    "        correct += int((pred == data.y).sum())  # Check against ground-truth labels.\n",
    "        misclass += list(data[pred != data.y])\n",
    "    return correct / len(loader.dataset), misclass  # Derive ratio of correct predictions.\n",
    "\n",
    "\n",
    "for epoch in range(1, 201):\n",
    "    train()\n",
    "    train_acc, trM = test(train_loader)\n",
    "    test_acc, testM = test(test_loader)\n",
    "#     print(f'Epoch: {epoch:03d}, Train Acc: {train_acc:.4f}')\n",
    "    if epoch % 1 == 0:\n",
    "        print(f'Epoch: {epoch:03d}, Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}')\n",
    "\n",
    "print(len(trM))\n",
    "for datapoint in trM:\n",
    "    print(datapoint.edge_index)\n",
    "    print(datapoint.y)\n",
    "        \n",
    "print(len(testM))\n",
    "for datapoint in testM:\n",
    "    print(datapoint.edge_index)\n",
    "    print(datapoint.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = GCNConv(1, 16)\n",
    "        self.conv2 = GCNConv(16, 1)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "\n",
    "        print(x)\n",
    "        print(x.shape)\n",
    "        x = self.conv1(x, edge_index)\n",
    "        print(x.shape)\n",
    "        x = F.relu(x)\n",
    "        print(x.shape)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        print(x.shape)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        print(x.shape)\n",
    "        x = F.sigmoid(x)\n",
    "        print(x.shape)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.]])\n",
      "torch.Size([96, 1])\n",
      "torch.Size([96, 16])\n",
      "torch.Size([96, 16])\n",
      "torch.Size([96, 16])\n",
      "torch.Size([96, 1])\n",
      "torch.Size([96, 1])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected input batch_size (96) to match target batch_size (32).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-44-e09219fa2867>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;31m#         print(out.shape)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;31m#         print(data.y.shape)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\bakalarka\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mnll_loss\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[0;32m   2259\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2260\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2261\u001b[1;33m         raise ValueError('Expected input batch_size ({}) to match target batch_size ({}).'\n\u001b[0m\u001b[0;32m   2262\u001b[0m                          .format(input.size(0), target.size(0)))\n\u001b[0;32m   2263\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mdim\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Expected input batch_size (96) to match target batch_size (32)."
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = Net().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "\n",
    "model.train()\n",
    "for epoch in range(200):\n",
    "    for data in loader:\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data)\n",
    "#         print(out.shape)\n",
    "#         print(data.y.shape)\n",
    "        loss = F.nll_loss(out, data.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "_, pred = model(data).max(dim=1)\n",
    "correct = int(pred[data.test_mask].eq(data.y[data.test_mask]).sum().item())\n",
    "acc = correct / int(data.test_mask.sum())\n",
    "print('Accuracy: {:.4f}'.format(acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bakalarka",
   "language": "python",
   "name": "bakalarka"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
